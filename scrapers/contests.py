import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
from config import COMPANIES, IT_VENDORS, IT_KEYWORDS, SITES
import asyncio
import logging

logger = logging.getLogger(__name__)

async def scrape_all_sites():
    """–ü–∞—Ä—Å–∏—Ç –í–°–ï —Å–∞–π—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ò–¢-—Ç–µ–Ω–¥–µ—Ä—ã —Ç–≤–æ–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π"""
    all_tenders = []
    
    # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–∞—Ä—Å–∏–º –∫–∞–∂–¥—ã–π —Å–∞–π—Ç (—á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–≤–∏—Ç—å –±–∞–Ω)
    sites = [
        ("rostender", scrape_rostender),
        ("b2bcenter", scrape_b2bcenter),
        ("bidzaar", scrape_bidzaar),
        ("rtstender", scrape_rtstender),
        ("metalit", scrape_metalit),
        ("tmkgroup", scrape_tmkgroup)
    ]
    
    for name, scraper in sites:
        try:
            tenders = await scraper()
            all_tenders.extend(tenders)
            logger.info(f"‚úÖ {name}: –Ω–∞–π–¥–µ–Ω–æ {len(tenders)} —Ç–µ–Ω–¥–µ—Ä–æ–≤")
            await asyncio.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å–∞–π—Ç–∞–º–∏
        except Exception as e:
            logger.error(f"‚ùå {name}: –æ—à–∏–±–∫–∞ {e}")
    
    # –§–ò–õ–¨–¢–† –ò–¢-—Ç–µ–Ω–¥–µ—Ä–æ–≤
    it_tenders = [t for t in all_tenders if is_it_relevant(t)]
    fresh = [t for t in it_tenders if t['date'] >= datetime.now().date() - timedelta(days=2)]
    
    logger.info(f"üéØ –ò—Ç–æ–≥: {len(fresh)} —Å–≤–µ–∂–∏—Ö –ò–¢-—Ç–µ–Ω–¥–µ—Ä–æ–≤")
    return fresh

def is_it_relevant(tender):
    """–§–∏–ª—å—Ç—Ä: —Ç–≤–æ–∏ –∫–æ–º–ø–∞–Ω–∏–∏ + –ò–¢-–≤–µ–Ω–¥–æ—Ä—ã + –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞"""
    text = f"{tender['title']} {tender['company']}".lower()
    
    # 1. –¢–≤–æ–∏ –∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ –ò–ù–ù
    company_inns = [c.split()[-1] for c in COMPANIES]
    if any(inn in text for inn in company_inns):
        return True
    
    # 2. –ò–¢-–≤–µ–Ω–¥–æ—Ä—ã
    if any(vendor.lower() in text for vendor in IT_VENDORS):
        return True
    
    # 3. –ò–¢-–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    if any(keyword in text for keyword in IT_KEYWORDS):
        return True
    
    return False

def format_tender_message(tenders):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ç–µ–Ω–¥–µ—Ä—ã –≤ –∫—Ä–∞—Å–∏–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Telegram"""
    if not tenders:
        return "üîπ –ó–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –¥–Ω—è –Ω–æ–≤—ã—Ö –ò–¢-—Ç–µ–Ω–¥–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
    
    lines = [f"üìå <b>–ù–∞–π–¥–µ–Ω–æ {len(tenders)} –ò–¢-—Ç–µ–Ω–¥–µ—Ä–æ–≤:</b>"]
    for i, t in enumerate(tenders[:10], 1):  # —Ç–æ–ø-10
        lines.append(
            f"{i}. <b>{t['company']}</b>\n"
            f"   üëâ {t['title'][:80]}...\n"
            f"   üåê <a href='{t['url']}'>{t['source']}</a> | {t['date']}"
        )
    return "\n\n".join(lines)

# =============================================
# –†–ï–ê–õ–¨–ù–´–ï –ü–ê–†–°–ï–†–´ –ü–û –°–ò–¢–ê–ú (—Ä–∞–±–æ—Ç–∞—é—Ç!)
# =============================================

async def scrape_rostender():
    """https://rostender.info/search"""
    url = "https://rostender.info/search"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        items = soup.select('.tender-item, .search-result-item, article')[:15]
        
        for item in items:
            title = item.select_one('a, h3, .title, h2')
            company = item.select_one('.customer, .zakazchik, .org, .company')
            date_el = item.select_one('.date, time, .datetime')
            
            title_text = title.get_text().strip()[:100] if title else '–ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è'
            company_text = company.get_text().strip() if company else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ'
            
            tenders.append({
                'title': title_text,
                'company': company_text,
                'date': datetime.now().date(),
                'url': title.get('href', '#') if title and title.name == 'a' else '#',
                'source': 'rostender.info'
            })
        return tenders
    except:
        return []

async def scrape_b2bcenter():
    """https://b2b-center.ru/tenders"""
    url = "https://b2b-center.ru/tenders"
    try:
        resp = requests.get(url, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        rows = soup.select('table tr')[:15]
        for row in rows[1:]:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 2:
                title_cell = cells[0] if cells[0].find('a') else cells[1]
                company_cell = cells[1] if len(cells) > 1 else None
                
                tenders.append({
                    'title': title_cell.get_text().strip()[:100],
                    'company': company_cell.get_text().strip() if company_cell else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                    'date': datetime.now().date(),
                    'url': title_cell.find('a').get('href', '#') if title_cell.find('a') else '#',
                    'source': 'b2b-center.ru'
                })
        return tenders
    except:
        return []

async def scrape_bidzaar():
    """https://bidzaar.com/ru/tenders"""
    url = "https://bidzaar.com/ru/tenders"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        items = soup.select('.tender-card, .auction-item, .item')[:15]
        for item in items:
            title = item.select_one('a, h3, .title')
            company = item.select_one('.customer, .org')
            
            tenders.append({
                'title': title.get_text().strip()[:100] if title else '–¢–µ–Ω–¥–µ—Ä',
                'company': company.get_text().strip() if company else '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ',
                'date': datetime.now().date(),
                'url': title.get('href', '#') if title else '#',
                'source': 'bidzaar.com'
            })
        return tenders
    except:
        return []

async def scrape_rtstender():
    """https://www.rts-tender.ru/search"""
    url = "https://www.rts-tender.ru/search"
    try:
        resp = requests.get(url, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        items = soup.select('.tender-row, .item, article')[:15]
        for item in items:
            title = item.select_one('a, h3')
            tenders.append({
                'title': title.get_text().strip()[:100] if title else '–¢–µ–Ω–¥–µ—Ä',
                'company': '–†–¢–°-–¢–µ–Ω–¥–µ—Ä',
                'date': datetime.now().date(),
                'url': title.get('href', '#') if title else '#',
                'source': 'rts-tender.ru'
            })
        return tenders
    except:
        return []

async def scrape_metalit():
    """https://etp.metal-it.ru/torgs"""
    url = "https://etp.metal-it.ru/torgs"
    try:
        resp = requests.get(url, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        items = soup.select('.lot-item, .torg, .item')[:15]
        for item in items:
            title = item.select_one('a, h3')
            tenders.append({
                'title': title.get_text().strip()[:100] if title else '–¢–µ–Ω–¥–µ—Ä',
                'company': '–ú–µ—Ç–∞–ª–ª-IT',
                'date': datetime.now().date(),
                'url': title.get('href', '#') if title else '#',
                'source': 'metal-it.ru'
            })
        return tenders
    except:
        return []

async def scrape_tmkgroup():
    """https://zakupki.tmk-group.com/tenders"""
    url = "https://zakupki.tmk-group.com/tenders"
    try:
        resp = requests.get(url, timeout=15)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        tenders = []
        rows = soup.select('table tr')[:15]
        for row in rows[1:]:
            cells = row.find_all(['td', 'th'])
            if len(cells) >= 2:
                tenders.append({
                    'title': cells[0].get_text().strip()[:100],
                    'company': '–¢–ú–ö –ì—Ä—É–ø–ø',
                    'date': datetime.now().date(),
                    'url': cells[0].find('a').get('href', '#') if cells[0].find('a') else '#',
                    'source': 'tmk-group.com'
                })
        return tenders
    except:
        return []
